{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysicsNeMo-Curator Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section contains a tutorial for using PhysicsNeMo-Curator to create a dataset.\n",
    "This tutorial will show how to use the PhysicsNeMo-Curator ETL pipeline to:\n",
    "\n",
    "1. Extract physics simulation data from a dataset\n",
    "2. Transform the data into an optimized, AI model training ready format\n",
    "3. Write the transformed data to disk efficiently\n",
    "\n",
    "## Create a dataset\n",
    "\n",
    "PhysicsNeMo-Curator works only with well-defined formats and schemas.\n",
    "As such, defining that is a necessary first step.\n",
    "Next, we'll create a custom dataset, in a custom schema, format and storage system.\n",
    "\n",
    "### Step 1: Define the schema, format, storage system\n",
    "\n",
    "For this tutorial, we'll create a simple simulation dataset using:\n",
    "\n",
    "**Format**: HDF5\n",
    "**Storage**: Local filesystem\n",
    "**Schema**: This is the structure for each simulation run\n",
    "(xyz indicates the run number):\n",
    "\n",
    "```bash\n",
    "run_xyz.h5\n",
    "├── /fields/\n",
    "│   ├── temperature          # Dataset: (N,) float32 - scalar temperature field\n",
    "│   └── velocity             # Dataset: (N, 3) float32 - 3D velocity vectors\n",
    "├── /geometry/\n",
    "│   └── coordinates          # Dataset: (N, 3) float32 - spatial coordinates (x,y,z)\n",
    "└── /metadata/\n",
    "    ├── timestamp            # Attribute: string - when simulation was run\n",
    "    ├── num_points           # Attribute: int - number of data points\n",
    "    ├── temperature_units    # Attribute: string - \"Kelvin\"\n",
    "    ├── velocity_units       # Attribute: string - \"m/s\"\n",
    "    └── simulation_params/   # Group containing simulation parameters\n",
    "        └── total_time      # Attribute: float - total simulation time\n",
    "```\n",
    "\n",
    "**Data Description**:\n",
    "\n",
    "- Each simulation run is one HDF5 file\n",
    "- `N` represents the number of spatial points in the simulation (varies per case)\n",
    "- Temperature is a scalar field representing thermal distribution\n",
    "- Velocity is a 3D vector field representing fluid flow\n",
    "- Coordinates define the spatial location of each data point\n",
    "- Metadata includes simulation parameters and units for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate random data\n",
    "\n",
    "We'll create a small script to generate 5 simulation runs with random data.\n",
    "Each file will contain about 1000 data points to keep it lightweight.\n",
    "\n",
    "First, we need to setup the environment.\n",
    "Let's install the dependencies necessary for this tutorial. This includes:\n",
    "\n",
    "1. Installing the `PhysicsNeMo-Curator` package itself\n",
    "2. h5py, numpy and zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/saikrishnanc/physics_nemo/physicsnemo-curator-saikrishnanc-fork\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/saikrishnanc/.local/lib/python3.10/site-packages (3.14.0)\n",
      "Requirement already satisfied: numpy in /home/saikrishnanc/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: zarr in /home/saikrishnanc/.local/lib/python3.10/site-packages (2.18.3)\n",
      "Requirement already satisfied: asciitree in /home/saikrishnanc/.local/lib/python3.10/site-packages (from zarr) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from zarr) (0.13.1)\n",
      "Requirement already satisfied: fasteners in /usr/lib/python3/dist-packages (from zarr) (0.14.1)\n",
      "Requirement already satisfied: pyvista>=0.44.2 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (0.44.2)\n",
      "Requirement already satisfied: vtk>=9.3.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (9.3.1)\n",
      "Requirement already satisfied: hydra-core>=1.3 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (4.67.1)\n",
      "Requirement already satisfied: pre-commit in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (4.2.0)\n",
      "Requirement already satisfied: black in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (25.1.0)\n",
      "Requirement already satisfied: ruff in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (0.11.8)\n",
      "Requirement already satisfied: interrogate in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (1.7.0)\n",
      "Requirement already satisfied: pytest in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (8.3.5)\n",
      "Requirement already satisfied: pytest-cov in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (6.1.1)\n",
      "Requirement already satisfied: pytest-mock in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (3.14.0)\n",
      "Requirement already satisfied: pytest-xdist in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (3.6.1)\n",
      "Requirement already satisfied: debugpy<1.9.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from physicsnemo_curator==0.0.1) (1.8.14)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from hydra-core>=1.3->physicsnemo_curator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/saikrishnanc/.local/lib/python3.10/site-packages (from hydra-core>=1.3->physicsnemo_curator==0.0.1) (4.9.3)\n",
      "Requirement already satisfied: packaging in /home/saikrishnanc/.local/lib/python3.10/site-packages (from hydra-core>=1.3->physicsnemo_curator==0.0.1) (24.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.10.1)\n",
      "Requirement already satisfied: pillow in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (11.1.0)\n",
      "Requirement already satisfied: pooch in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.8.2)\n",
      "Requirement already satisfied: scooby>=0.5.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pyvista>=0.44.2->physicsnemo_curator==0.0.1) (4.12.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from black->physicsnemo_curator==0.0.1) (8.1.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from black->physicsnemo_curator==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from black->physicsnemo_curator==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from black->physicsnemo_curator==0.0.1) (4.3.7)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from black->physicsnemo_curator==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: attrs in /usr/lib/python3/dist-packages (from interrogate->physicsnemo_curator==0.0.1) (19.3.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from interrogate->physicsnemo_curator==0.0.1) (0.4.3)\n",
      "Requirement already satisfied: py in /home/saikrishnanc/.local/lib/python3.10/site-packages (from interrogate->physicsnemo_curator==0.0.1) (1.11.0)\n",
      "Requirement already satisfied: tabulate in /home/saikrishnanc/.local/lib/python3.10/site-packages (from interrogate->physicsnemo_curator==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pre-commit->physicsnemo_curator==0.0.1) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pre-commit->physicsnemo_curator==0.0.1) (2.6.9)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pre-commit->physicsnemo_curator==0.0.1) (1.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from pre-commit->physicsnemo_curator==0.0.1) (5.3.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pre-commit->physicsnemo_curator==0.0.1) (20.30.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pytest->physicsnemo_curator==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pytest->physicsnemo_curator==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pytest->physicsnemo_curator==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: coverage>=7.5 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from coverage[toml]>=7.5->pytest-cov->physicsnemo_curator==0.0.1) (7.8.0)\n",
      "Requirement already satisfied: execnet>=2.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from pytest-xdist->physicsnemo_curator==0.0.1) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->physicsnemo_curator==0.0.1) (0.3.9)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->physicsnemo_curator==0.0.1) (3.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from pooch->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (2.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/saikrishnanc/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista>=0.44.2->physicsnemo_curator==0.0.1) (1.16.0)\n",
      "Building wheels for collected packages: physicsnemo_curator\n",
      "  Building editable for physicsnemo_curator (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for physicsnemo_curator: filename=physicsnemo_curator-0.0.1-0.editable-py3-none-any.whl size=10122 sha256=5e2d88f013da7b72aefefa1494c5e8ce9542846890aa638c1860bbff1838ebb6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tw6uxs8e/wheels/09/89/2d/4a5e09b2bf48d31543b554fdec354442543b70249d40e9e7eb\n",
      "Successfully built physicsnemo_curator\n",
      "Installing collected packages: physicsnemo_curator\n",
      "  Attempting uninstall: physicsnemo_curator\n",
      "    Found existing installation: physicsnemo_curator 0.0.1\n",
      "    Uninstalling physicsnemo_curator-0.0.1:\n",
      "      Successfully uninstalled physicsnemo_curator-0.0.1\n",
      "Successfully installed physicsnemo_curator-0.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e \"../../../[dev]\" h5py numpy zarr --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate some random data, that adheres to the schema we've developed above.\n",
    "The below code snippet can also be found as a script in [./generate_sample_data.py](./generate_sample_data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample physics simulation dataset...\n",
      "Created tutorial_data/run_001.h5 with 1000 data points\n",
      "Created tutorial_data/run_002.h5 with 1000 data points\n",
      "Created tutorial_data/run_003.h5 with 1000 data points\n",
      "Created tutorial_data/run_004.h5 with 1000 data points\n",
      "Created tutorial_data/run_005.h5 with 1000 data points\n",
      "\n",
      "Dataset generation complete!\n",
      "Created 5 HDF5 files in the 'tutorial_data/' directory\n",
      "Each file contains ~1000 data points with temperature and velocity fields\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_simulation_data(num_points=1000):\n",
    "    \"\"\"Generate random simulation data for one run.\"\"\"\n",
    "\n",
    "    # Generate random 3D coordinates in a unit cube\n",
    "    coordinates = np.random.uniform(-1.0, 1.0, size=(num_points, 3)).astype(np.float32)\n",
    "\n",
    "    # Generate temperature field (scalar, range 250-350 K)\n",
    "    temperature = np.random.uniform(250.0, 350.0, size=num_points).astype(np.float32)\n",
    "\n",
    "    # Generate velocity field (3D vectors, range -5 to 5 m/s)\n",
    "    velocity = np.random.uniform(-5.0, 5.0, size=(num_points, 3)).astype(np.float32)\n",
    "\n",
    "    return coordinates, temperature, velocity\n",
    "\n",
    "def create_hdf5_file(run_number, output_dir=\"tutorial_data\"):\n",
    "    \"\"\"Create one HDF5 file for a simulation run.\"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate data\n",
    "    coordinates, temperature, velocity = generate_simulation_data(run_number)\n",
    "    num_points = len(coordinates)\n",
    "\n",
    "    # Create HDF5 file\n",
    "    filename = f\"run_{run_number:03d}.h5\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        # Create groups\n",
    "        fields_group = f.create_group('fields')\n",
    "        geometry_group = f.create_group('geometry')\n",
    "        metadata_group = f.create_group('metadata')\n",
    "        sim_params_group = metadata_group.create_group('simulation_params')\n",
    "\n",
    "        # Store field data\n",
    "        fields_group.create_dataset('temperature', data=temperature)\n",
    "        fields_group.create_dataset('velocity', data=velocity)\n",
    "\n",
    "        # Store geometry data\n",
    "        geometry_group.create_dataset('coordinates', data=coordinates)\n",
    "\n",
    "        # Store metadata attributes\n",
    "        metadata_group.attrs['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        metadata_group.attrs['num_points'] = num_points\n",
    "        metadata_group.attrs['temperature_units'] = 'Kelvin'\n",
    "        metadata_group.attrs['velocity_units'] = 'm/s'\n",
    "\n",
    "        # Store simulation parameters\n",
    "        sim_params_group.attrs['total_time'] = np.random.uniform(1.0, 10.0)  # Random simulation time\n",
    "\n",
    "    print(f\"Created {filepath} with {num_points} data points\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate sample dataset with 5 simulation runs.\"\"\"\n",
    "    print(\"Generating sample physics simulation dataset...\")\n",
    "\n",
    "    # Generate 5 runs\n",
    "    for run_num in range(1, 6):\n",
    "        create_hdf5_file(run_num)\n",
    "\n",
    "    print(\"\\nDataset generation complete!\")\n",
    "    print(\"Created 5 HDF5 files in the 'tutorial_data/' directory\")\n",
    "    print(\"Each file contains ~1000 data points with temperature and velocity fields\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `tutorial_data/` directory with 5 files.\n",
    "Now we're ready to implement the ETL pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the ETL pipeline\n",
    "\n",
    "The PhysicsNeMo-Curator ETL pipeline consists of four main components:\n",
    "\n",
    "1. **DataSource** - Handles both reading input data AND writing output data\n",
    "(serves as both source and sink)\n",
    "2. **DataTransformation** - Transforms data from one format to another\n",
    "3. **DatasetValidator** - Validates input data structure and content (optional)\n",
    "4. **ParallelProcessor** - Orchestrates the processing files in parallel\n",
    "\n",
    "**For this tutorial, our specific pipeline will be:**\n",
    "\n",
    "1. **H5DataSource** (source) - Reads HDF5 files and extracts raw data\n",
    "2. **H5ToZarrTransformation** - Converts it to a Zarr-compatible format\n",
    "3. **ZarrDataSource** (sink) - Writes the transformed data to Zarr stores\n",
    "4. **TutorialValidator** - Validates our HDF5 input files\n",
    "\n",
    "The data flow:\n",
    "`HDF5 files → H5DataSource → H5ToZarrTransformation → ZarrDataSource → Zarr stores`\n",
    "\n",
    "**Important**:\n",
    "Notice that we use different DataSource classes for reading and writing -\n",
    "one specialized for HDF5 input, another for Zarr output.\n",
    "This shows how you can mix and match different data sources in the same pipeline.\n",
    "However, notice also that `DataSource` serves dual purposes -\n",
    "one instance of a class can read your input data, while another\n",
    "instance of the same class can write your output data.\n",
    "This design allows the same class to handle both ends of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Implement dataset validation\n",
    "\n",
    "First, we'll implement validation to ensure our input HDF5 files\n",
    "meet the required schema and format.\n",
    "This runs at the beginning of the pipeline to catch issues early.\n",
    "The below code snippet can also be found as a script in [./tutorial_validator.py](./tutorial_validator.py).\n",
    "\n",
    "NOTE: The intention is to run this as part of the pipeline, and not as a standalone script.\n",
    "If you'd like to run it as a separate script, please modify the execution accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import h5py  # noqa: F811\n",
    "\n",
    "from physicsnemo_curator.etl.dataset_validators import (\n",
    "    DatasetValidator,\n",
    "    ValidationError,\n",
    "    ValidationLevel,\n",
    ")\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class TutorialValidator(DatasetValidator):\n",
    "    \"\"\"Validator for HDF5 physics simulation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, input_dir: str, validation_level: str = \"fields\"):\n",
    "        \"\"\"Initialize the validator.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            input_dir: Directory containing HDF5 files to validate\n",
    "            validation_level: \"structure\" or \"fields\"\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.validation_level = ValidationLevel(validation_level)\n",
    "\n",
    "        # Define our expected schema\n",
    "        self.required_groups = ['/fields', '/geometry', '/metadata', '/metadata/simulation_params']\n",
    "        self.required_datasets = {\n",
    "            '/fields/temperature': {'shape_dims': 1, 'dtype': 'float'},\n",
    "            '/fields/velocity': {'shape_dims': 2, 'expected_cols': 3, 'dtype': 'float'},\n",
    "            '/geometry/coordinates': {'shape_dims': 2, 'expected_cols': 3, 'dtype': 'float'}\n",
    "        }\n",
    "        self.required_attributes = {\n",
    "            '/metadata': ['timestamp', 'num_points', 'temperature_units', 'velocity_units'],\n",
    "            '/metadata/simulation_params': ['total_time']\n",
    "        }\n",
    "\n",
    "    def validate(self) -> List[ValidationError]:\n",
    "        \"\"\"Validate the entire dataset.\n",
    "\n",
    "        Returns:\n",
    "            List of validation errors (empty if validation passes)\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check if input directory exists\n",
    "        if not self.input_dir.exists():\n",
    "            errors.append(ValidationError(\n",
    "                path=self.input_dir,\n",
    "                message=f\"Input directory does not exist: {self.input_dir}\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "            return errors\n",
    "\n",
    "        # Find all HDF5 files\n",
    "        h5_files = list(self.input_dir.glob(\"*.h5\"))\n",
    "\n",
    "        if not h5_files:\n",
    "            errors.append(ValidationError(\n",
    "                path=self.input_dir,\n",
    "                message=\"No HDF5 files found in input directory\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "            return errors\n",
    "\n",
    "        # Validate each file\n",
    "        for h5_file in h5_files:\n",
    "            file_errors = self.validate_single_item(h5_file)\n",
    "            errors.extend(file_errors)\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def validate_single_item(self, item: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate a single HDF5 file.\n",
    "\n",
    "        Args:\n",
    "            item: Path to HDF5 file to validate\n",
    "\n",
    "        Returns:\n",
    "            List of validation errors for this file\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "\n",
    "        try:\n",
    "            with h5py.File(item, 'r') as f:\n",
    "                # Structure validation\n",
    "                errors.extend(self._validate_structure(f, item))\n",
    "\n",
    "                # Field validation (if requested and structure is valid)\n",
    "                if self.validation_level == ValidationLevel.FIELDS and not errors:\n",
    "                    errors.extend(self._validate_fields(f, item))\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append(ValidationError(\n",
    "                path=item,\n",
    "                message=f\"Failed to open HDF5 file: {str(e)}\",\n",
    "                level=self.validation_level\n",
    "            ))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_structure(self, f: h5py.File, file_path: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate HDF5 file structure.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check required groups exist\n",
    "        errors.extend([\n",
    "            ValidationError(\n",
    "                path=file_path,\n",
    "                message=f\"Missing required group: {group_path}\",\n",
    "                level=self.validation_level,\n",
    "            )\n",
    "            for group_path in self.required_groups\n",
    "            if group_path not in f\n",
    "        ])\n",
    "\n",
    "        # Check required datasets exist and have correct structure\n",
    "        for dataset_path, requirements in self.required_datasets.items():\n",
    "            if dataset_path not in f:\n",
    "                errors.append(\n",
    "                    ValidationError(\n",
    "                        path=file_path,\n",
    "                        message=f\"Missing required dataset: {dataset_path}\",\n",
    "                        level=self.validation_level,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            dataset = f[dataset_path]\n",
    "\n",
    "            # Check dimensions\n",
    "            if len(dataset.shape) != requirements[\"shape_dims\"]:\n",
    "                errors.append(\n",
    "                    ValidationError(\n",
    "                        path=file_path,\n",
    "                        message=f\"Dataset {dataset_path} has wrong dimensions: expected {requirements['shape_dims']}D, got {len(dataset.shape)}D\",\n",
    "                        level=self.validation_level,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Check column count for 2D arrays\n",
    "            if \"expected_cols\" in requirements and len(dataset.shape) >= 2:\n",
    "                if dataset.shape[1] != requirements[\"expected_cols\"]:\n",
    "                    errors.append(\n",
    "                        ValidationError(\n",
    "                            path=file_path,\n",
    "                            message=f\"Dataset {dataset_path} has wrong number of columns: expected {requirements['expected_cols']}, got {dataset.shape[1]}\",\n",
    "                            level=self.validation_level,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Check required attributes exist\n",
    "        errors.extend([\n",
    "            ValidationError(\n",
    "                path=file_path,\n",
    "                message=f\"Missing required attribute: {group_path}@{attr_name}\",\n",
    "                level=self.validation_level,\n",
    "            )\n",
    "            for group_path, attr_list in self.required_attributes.items()\n",
    "            if group_path in f\n",
    "            for attr_name in attr_list\n",
    "            if attr_name not in f[group_path].attrs\n",
    "        ])\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_fields(self, f: h5py.File, file_path: Path) -> List[ValidationError]:\n",
    "        \"\"\"Validate field data content.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check that datasets have consistent sizes\n",
    "        if '/fields/temperature' in f and '/geometry/coordinates' in f:\n",
    "            temp_size = f['/fields/temperature'].shape[0]\n",
    "            coord_size = f['/geometry/coordinates'].shape[0]\n",
    "\n",
    "            if temp_size != coord_size:\n",
    "                errors.append(ValidationError(\n",
    "                    path=file_path,\n",
    "                    message=f\"Inconsistent data sizes: temperature has {temp_size} points, coordinates has {coord_size} points\",\n",
    "                    level=self.validation_level\n",
    "                ))\n",
    "\n",
    "        # Check for reasonable data ranges\n",
    "        if '/fields/temperature' in f:\n",
    "            temp_data = f['/fields/temperature'][:]\n",
    "            if temp_data.min() < 0 or temp_data.max() > 10000:  # Kelvin range check\n",
    "                errors.append(ValidationError(\n",
    "                    path=file_path,\n",
    "                    message=f\"Temperature data out of reasonable range: [{temp_data.min():.1f}, {temp_data.max():.1f}] K\",\n",
    "                    level=self.validation_level\n",
    "                ))\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement data source\n",
    "\n",
    "We'll create a simple DataSource that reads our HDF5 files. \n",
    "The below code snippet can also be found as a script in [./h5_data_source.py](./h5_data_source.py).\n",
    "\n",
    "**Note**: This DataSource only implements reading from HDF5 files, since the idea here is for the source to be read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import h5py  # noqa: F811\n",
    "import numpy as np  # noqa: F811\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class H5DataSource(DataSource):\n",
    "    \"\"\"DataSource for reading HDF5 physics simulation files.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, input_dir: str):\n",
    "        \"\"\"Initialize the H5 data source.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            input_dir: Directory containing input HDF5 files\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.input_dir = Path(input_dir)\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            raise FileNotFoundError(f\"Input directory {self.input_dir} does not exist\")\n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Get list of HDF5 files to process.\n",
    "\n",
    "        Returns:\n",
    "            List of filenames (without extension) to process\n",
    "        \"\"\"\n",
    "        h5_files = list(self.input_dir.glob(\"*.h5\"))\n",
    "        filenames = [f.stem for f in h5_files]  # Remove .h5 extension\n",
    "\n",
    "        self.logger.info(f\"Found {len(filenames)} HDF5 files to process\")\n",
    "        return sorted(filenames)\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Read one HDF5 file and extract all data.\n",
    "\n",
    "        Args:\n",
    "            filename: Base filename (without extension)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing extracted data and metadata\n",
    "        \"\"\"\n",
    "        filepath = self.input_dir / f\"{filename}.h5\"\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "        self.logger.info(f\"Reading {filepath}\")\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Read field data\n",
    "            data['temperature'] = np.array(f['fields/temperature'])\n",
    "            data['velocity'] = np.array(f['fields/velocity'])\n",
    "\n",
    "            # Read geometry data\n",
    "            data['coordinates'] = np.array(f['geometry/coordinates'])\n",
    "\n",
    "            # Read metadata\n",
    "            metadata = dict(f[\"metadata\"].attrs.items())\n",
    "\n",
    "            data['metadata'] = metadata\n",
    "            data['filename'] = filename\n",
    "\n",
    "        self.logger.info(f\"Loaded data with {len(data['temperature'])} points\")\n",
    "        return data\n",
    "\n",
    "    def write(self, data: Dict[str, Any], filename: str) -> None:\n",
    "        \"\"\"Not implemented - this DataSource only reads.\"\"\"\n",
    "        raise NotImplementedError(\"H5DataSource only supports reading\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        \"\"\"Never skip files for reading.\"\"\"\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement transformations\n",
    "\n",
    "Now we'll create a transformation that converts our HDF5 data into a format optimized for Zarr storage. \n",
    "The below code snippet can also be found as a script in [./h5_to_zarr_transformation.py](./h5_to_zarr_transformation.py).\n",
    "\n",
    "**Key Points About This Transformation:**\n",
    "\n",
    "1. **Zarr Optimization**: Prepares data with chunks, compression, and proper dtypes for efficient Zarr storage\n",
    "\n",
    "2. **Chunking Strategy**: Uses configurable chunk sizes optimized for the data size\n",
    "\n",
    "3. **Compression**: Uses zstd compression with Blosc for good performance/size balance\n",
    "\n",
    "4. **Derived data**: Adds derived fields like velocity magnitude and temperature statistics\n",
    "\n",
    "5. **Metadata**: Adds technical metadata about chunking and compression settings\n",
    "\n",
    "The output format is specifically designed to be consumed by a `ZarrDataSource` that will create the actual Zarr store structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np  # noqa: F811\n",
    "from numcodecs import Blosc\n",
    "\n",
    "from physicsnemo_curator.etl.data_transformations import DataTransformation\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class H5ToZarrTransformation(DataTransformation):\n",
    "    \"\"\"Transform HDF5 data into Zarr-optimized format.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, chunk_size: int = 500, compression_level: int = 3):\n",
    "        \"\"\"Initialize the transformation.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            chunk_size: Chunk size for Zarr arrays (number of points per chunk)\n",
    "            compression_level: Compression level (1-9, higher = more compression)\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.compression_level = compression_level\n",
    "\n",
    "        # Set up compression\n",
    "        self.compressor = Blosc(\n",
    "            cname='zstd',  # zstd compression algorithm\n",
    "            clevel=compression_level,\n",
    "            shuffle=Blosc.SHUFFLE\n",
    "        )\n",
    "\n",
    "    def transform(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Transform HDF5 data to Zarr-optimized format.\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary from H5DataSource.read_file()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with Zarr-optimized arrays and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Transforming {data['filename']} for Zarr storage\")\n",
    "\n",
    "        # Get the number of points to determine chunking\n",
    "        num_points = len(data['temperature'])\n",
    "\n",
    "        # Calculate optimal chunks (don't exceed chunk_size)\n",
    "        chunk_points = min(self.chunk_size, num_points)\n",
    "\n",
    "        # Prepare arrays that will be written to Zarr stores\n",
    "        zarr_data = {\n",
    "            'temperature': {},\n",
    "            'velocity': {},\n",
    "            'coordinates': {},\n",
    "            'velocity_magnitude': {},\n",
    "        }\n",
    "\n",
    "        # Temperature field (1D array)\n",
    "        zarr_data['temperature'] = {\n",
    "            'data': data['temperature'].astype(np.float32),\n",
    "            'chunks': (chunk_points,),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Velocity field (2D array: points x 3 components)\n",
    "        zarr_data['velocity'] = {\n",
    "            'data': data['velocity'].astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Coordinates (2D array: points x 3 dimensions)\n",
    "        zarr_data['coordinates'] = {\n",
    "            'data': data['coordinates'].astype(np.float32),\n",
    "            'chunks': (chunk_points, 3),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "\n",
    "        # Add some computed metadata useful for Zarr to existing metadata\n",
    "        metadata = data['metadata']\n",
    "        metadata['num_points'] = num_points\n",
    "        metadata['chunk_size'] = chunk_points\n",
    "        metadata['compression'] = 'zstd'\n",
    "        metadata['compression_level'] = self.compression_level\n",
    "\n",
    "        # Also add some simple derived fields\n",
    "        # Temperature statistics\n",
    "        metadata['temperature_min'] = float(np.min(data['temperature']))\n",
    "        metadata['temperature_max'] = float(np.max(data['temperature']))\n",
    "        metadata['temperature_mean'] = float(np.mean(data['temperature']))\n",
    "\n",
    "        # Velocity magnitude\n",
    "        velocity_magnitude = np.linalg.norm(data['velocity'], axis=1)\n",
    "        zarr_data['velocity_magnitude'] = {\n",
    "            'data': velocity_magnitude.astype(np.float32),\n",
    "            'chunks': (chunk_points,),\n",
    "            'compressor': self.compressor,\n",
    "            'dtype': np.float32\n",
    "        }\n",
    "        metadata['velocity_max'] = float(np.max(velocity_magnitude))\n",
    "        zarr_data['metadata'] = metadata\n",
    "\n",
    "        return zarr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement sink\n",
    "\n",
    "Now we'll create a DataSource that writes to Zarr stores.\n",
    "Each simulation run will be stored as a separate Zarr store for efficient individual access.\n",
    "The following code snippet can also be found in [./zarr_data_source.py](./zarr_data_source.py).\n",
    "\n",
    "**Key Points About This Sink:**\n",
    "\n",
    "1. **Individual Stores**: Each simulation run gets its own `.zarr` directory for efficient access\n",
    "\n",
    "2. **Optimized Storage**: Uses the chunking and compression settings from the transformation\n",
    "\n",
    "3. **Complete Metadata**: Stores all metadata as Zarr attributes for easy access\n",
    "\n",
    "4. **Overwrite Control**: Configurable overwrite behavior for reprocessing workflows\n",
    "\n",
    "5. **Write-Only**: This sink only writes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import zarr\n",
    "\n",
    "from physicsnemo_curator.etl.data_sources import DataSource\n",
    "from physicsnemo_curator.etl.processing_config import ProcessingConfig\n",
    "\n",
    "\n",
    "class ZarrDataSource(DataSource):\n",
    "    \"\"\"DataSource for writing to Zarr stores.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ProcessingConfig, output_dir: str):\n",
    "        \"\"\"Initialize the Zarr data source.\n",
    "\n",
    "        Args:\n",
    "            cfg: Processing configuration\n",
    "            output_dir: Directory to write Zarr stores\n",
    "        \"\"\"\n",
    "        super().__init__(cfg)\n",
    "        self.output_dir = Path(output_dir)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_file_list(self) -> List[str]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def read_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Not implemented - this DataSource only writes.\"\"\"\n",
    "        raise NotImplementedError(\"ZarrDataSource only supports writing\")\n",
    "\n",
    "    def write(self, data: Dict[str, Any], filename: str) -> None:\n",
    "        \"\"\"Write transformed data to a Zarr store.\n",
    "\n",
    "        Args:\n",
    "            data: Transformed data from H5ToZarrTransformation\n",
    "            filename: Base filename for the Zarr store\n",
    "        \"\"\"\n",
    "        store_path = self.output_dir / f\"{filename}.zarr\"\n",
    "\n",
    "        if store_path.exists():\n",
    "            self.logger.info(f\"Skipping {filename} - Zarr store already exists\")\n",
    "            return\n",
    "\n",
    "        # Create Zarr store\n",
    "        self.logger.info(f\"Creating Zarr store: {store_path}\")\n",
    "        store = zarr.DirectoryStore(store_path)\n",
    "        root = zarr.group(store=store)\n",
    "\n",
    "        # Store metadata as root attributes\n",
    "        if \"metadata\" in data:\n",
    "            for key, value in data[\"metadata\"].items():\n",
    "                # Convert numpy types to Python types for JSON serialization\n",
    "                if hasattr(value, \"item\"):  # numpy scalar\n",
    "                    value = value.item()\n",
    "                root.attrs[key] = value\n",
    "            data.pop(\"metadata\")\n",
    "\n",
    "        # Write all arrays from the transformation\n",
    "        for array_name, array_info in data.items():\n",
    "            root.create_dataset(\n",
    "                array_name,\n",
    "                data=array_info[\"data\"],\n",
    "                chunks=array_info[\"chunks\"],\n",
    "                compressor=array_info[\"compressor\"],\n",
    "                dtype=array_info[\"dtype\"]\n",
    "            )\n",
    "\n",
    "        # Add some store-level metadata\n",
    "        root.attrs[\"zarr_format\"] = 2\n",
    "        root.attrs[\"created_by\"] = \"physicsnemo-curator-tutorial\"\n",
    "\n",
    "        # Something weird is happening here.\n",
    "        # If this error occurs, the stores are created and we move to the next one.\n",
    "        # If this error does NOT occur, we seem to skip all the remaining files.\n",
    "        # Debug with Alexey.\n",
    "        self.logger.info(\"Successfully created Zarr store\")\n",
    "\n",
    "    def should_skip(self, filename: str) -> bool:\n",
    "        \"\"\"Check if we should skip writing this store.\n",
    "\n",
    "        Args:\n",
    "            filename: Base filename to check\n",
    "\n",
    "        Returns:\n",
    "            True if store should be skipped (already exists)\n",
    "        \"\"\"\n",
    "        store_path = self.output_dir / f\"{filename}.zarr\"\n",
    "        exists = store_path.exists()\n",
    "\n",
    "        if exists:\n",
    "            self.logger.info(f\"Skipping {filename} - Zarr store already exists\")\n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run the ETL pipeline\n",
    "\n",
    "Now we'll tie everything together with a configuration file and run the complete pipeline. This config is present in [./tutorial_config.yaml](./tutorial_config.yaml).\n",
    "\n",
    "```yaml\n",
    "# Tutorial ETL Pipeline Configuration\n",
    "# This demonstrates the complete H5 -> Zarr processing pipeline\n",
    "\n",
    "etl:\n",
    "  # Processing settings\n",
    "  processing:\n",
    "    num_processes: 2  # Use 2 processes for this small tutorial dataset\n",
    "    args: {}\n",
    "\n",
    "  # Validation (runs first)\n",
    "  validator:\n",
    "    _target_: tutorial_validator.TutorialValidator\n",
    "    _convert_: all\n",
    "    input_dir: ???  # Will be provided via command line\n",
    "    validation_level: \"fields\"  # Full validation including data content\n",
    "\n",
    "  # Source (reads HDF5 files)\n",
    "  source:\n",
    "    _target_: h5_data_source.H5DataSource\n",
    "    _convert_: all\n",
    "    input_dir: ???  # Will be provided via command line\n",
    "\n",
    "  # Transformations (convert to Zarr format)\n",
    "  transformations:\n",
    "    h5_to_zarr:\n",
    "      _target_: h5_to_zarr_transformation.H5ToZarrTransformation\n",
    "      _convert_: all\n",
    "      chunk_size: 500\n",
    "      compression_level: 3\n",
    "\n",
    "  # Sink (writes Zarr stores)\n",
    "  sink:\n",
    "    _target_: zarr_data_source.ZarrDataSource\n",
    "    _convert_: all\n",
    "    output_dir: ???  # Will be provided via command line\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the ETL Pipeline:**\n",
    "\n",
    "Now you can run the complete pipeline using the `physicsnemo-curator` CLI.\n",
    "\n",
    "**IMPORTANT** Make sure your tutorial files are importable (and that the paths are correct).\n",
    "The below command attempts to add the correct directories to the `PYTHONPATH`, but please modify this is you've changed the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 16:40:00,538][physicsnemo_curator.utils.utils][INFO] - Starting ETL pipeline\n",
      "[2025-07-21 16:40:00,540][physicsnemo_curator.utils.utils][INFO] - Config summary:\n",
      "etl:\n",
      "  processing:\n",
      "    args: {}\n",
      "    num_processes: 2\n",
      "  sink:\n",
      "    _convert_: all\n",
      "    _target_: zarr_data_source.ZarrDataSource\n",
      "    output_dir: output_zarr\n",
      "  source:\n",
      "    _convert_: all\n",
      "    _target_: h5_data_source.H5DataSource\n",
      "    input_dir: tutorial_data\n",
      "  transformations:\n",
      "    h5_to_zarr:\n",
      "      _convert_: all\n",
      "      _target_: h5_to_zarr_transformation.H5ToZarrTransformation\n",
      "      chunk_size: 500\n",
      "      compression_level: 3\n",
      "  validator:\n",
      "    _convert_: all\n",
      "    _target_: tutorial_validator.TutorialValidator\n",
      "    input_dir: ???\n",
      "    validation_level: fields\n",
      "\n",
      "[2025-07-21 16:40:00,680][H5DataSource][INFO] - Found 5 HDF5 files to process\n",
      "Processing files: 100%|█████████████████████████| 5/5 [00:00<00:00,  9.91file/s]\n",
      "[2025-07-21 16:40:01,186][physicsnemo_curator.utils.utils][INFO] - \n",
      "Processing Summary:\n",
      "[2025-07-21 16:40:01,186][physicsnemo_curator.utils.utils][INFO] - Number of processes: 2\n",
      "[2025-07-21 16:40:01,186][physicsnemo_curator.utils.utils][INFO] - Total wall clock time: 0.51 seconds\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=$PYTHONPATH:./ physicsnemo-curator-etl --config-dir ./ \\\n",
    "  --config-name tutorial_config \\\n",
    "  etl.source.input_dir=tutorial_data \\\n",
    "  etl.sink.output_dir=output_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Execution:**\n",
    "\n",
    "1. **Validation Phase**: The pipeline first validates all HDF5 files in `tutorial_data/`\n",
    "   - Checks file structure and schema compliance\n",
    "   - Validates data ranges and consistency\n",
    "   - Stops execution if any validation errors are found\n",
    "\n",
    "2. **Processing Phase**: For each validated file:\n",
    "   - H5DataSource reads the HDF5 file\n",
    "   - H5ToZarrTransformation converts it to Zarr-optimized format\n",
    "   - ZarrDataSource writes the result to a `.zarr` store\n",
    "\n",
    "3. **Parallel Execution**: Uses 2 processes to handle multiple files simultaneously\n",
    "\n",
    "4. **Output**: Creates individual Zarr stores for each input file\n",
    "\n",
    "**Expected Output Structure:**\n",
    "\n",
    "After running, you'll have:\n",
    "\n",
    "```bash\n",
    "output_zarr/\n",
    "├── run_001.zarr/\n",
    "│   ├── temperature/\n",
    "│   ├── velocity/\n",
    "│   ├── coordinates/\n",
    "│   ├── velocity_magnitude/\n",
    "│   └── .zattrs (metadata)\n",
    "├── run_002.zarr/\n",
    "├── run_003.zarr/\n",
    "├── run_004.zarr/\n",
    "└── run_005.zarr/\n",
    "```\n",
    "\n",
    "**Verify the Results:**\n",
    "\n",
    "You can inspect the output using this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays in store: ['coordinates', 'temperature', 'velocity', 'velocity_magnitude']\n",
      "Temperature data shape: (1000,)\n",
      "Velocity data shape: (1000, 3)\n",
      "Metadata: {'chunk_size': 500, 'compression': 'zstd', 'compression_level': 3, 'created_by': 'physicsnemo-curator-tutorial', 'num_points': 1000, 'temperature_max': 349.88116455078125, 'temperature_mean': 300.00732421875, 'temperature_min': 250.10169982910156, 'temperature_units': 'Kelvin', 'timestamp': '2025-07-21 23:15:25', 'velocity_max': 8.128337860107422, 'velocity_units': 'm/s', 'zarr_format': 2}\n",
      "Temperature range: 250.1 - 349.9 K\n"
     ]
    }
   ],
   "source": [
    "import zarr  # noqa: F811\n",
    "\n",
    "# Open a Zarr store\n",
    "store = zarr.open(\"output_zarr/run_001.zarr\", mode=\"r\")\n",
    "\n",
    "print(\"Arrays in store:\", list(store.keys()))\n",
    "print(\"Temperature data shape:\", store[\"temperature\"].shape)\n",
    "print(\"Velocity data shape:\", store[\"velocity\"].shape)\n",
    "print(\"Metadata:\", dict(store.attrs))\n",
    "\n",
    "# Access the data\n",
    "temperature = store[\"temperature\"][:]\n",
    "print(f\"Temperature range: {temperature.min():.1f} - {temperature.max():.1f} K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
